{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import email\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "import networkx as nx\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning global variables and declaring lists\n",
    "#Loading Spacy English Model for tokenization and Page Rank\n",
    "try:\n",
    "    spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "textrank = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "#List of Part-of-speech tokens, the corresponding tokens can be disregarded\n",
    "pos_tokens_unused = ['NUM','SYM','PUNCT','X','SPACE']\n",
    "#List of commonly used Email abbrevations and phrases that indicates priority\n",
    "email_abbrevations = ['AEAP', 'ASAP', 'AR', 'Action Required','FYA', 'NYR', 'NYRT','NYRQ', \n",
    "                      'Quick','attention','critical','immediate','urgent','priority']\n",
    "#List of Senders picked from Data Pre-processing notebook\n",
    "email_from = ['exchangeinfo@nymex.com@ENRON', 'Leslie Hansen', 'Sara Shackleton', 'Mark Taylor','Marcus Nettelton']\n",
    "#Weights given for abbrevations and senders\n",
    "abbrevation_priority = 0.8\n",
    "email_from_priority = 0.5\n",
    "#Get the GloVe vector word embeddings from Stanford NLP library\n",
    "def generate_word_embeddings():\n",
    "    if not path.exists('glove.6B.zip'):\n",
    "        print('Please wait while the Global Vector Word Embeddings are being downloaded.')\n",
    "        !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "        !unzip glove*.zip\n",
    "    word_embeddings = {}\n",
    "    with open('glove.6B.100d.txt',encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            word = words[0]\n",
    "            word_vectors = np.asarray(words[1:], dtype='float32')\n",
    "            word_embeddings[word] = word_vectors\n",
    "    return word_embeddings\n",
    "word_embeddings = generate_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the pre-processed data and dropping out unwanted columns.\n",
    "emails_data = pd.read_csv('emails_filtered.csv')\n",
    "emails_data.drop(['Unnamed: 0','message-id'],inplace=True,axis=1)\n",
    "emails_data.fillna(value='',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get contents of forward and original messages and store it in a list for further processing\n",
    "def get_payloads(emails,recieved_email=False):\n",
    "    payloads = [emails[0].strip()]\n",
    "    if recieved_email:\n",
    "        return ''.join(payloads)\n",
    "    for mail in emails[1:]:\n",
    "        mail = email.message_from_string(mail.strip())\n",
    "        mail  = mail.get_payload()\n",
    "        if mail == None or mail == '':\n",
    "            continue\n",
    "        else:\n",
    "            payloads.append(mail)\n",
    "    return payloads\n",
    "\n",
    "#Function to handle abbrevations in email subject\n",
    "def handle_abbrevations(subject_data):\n",
    "    return re.sub(\"[R|r][e|E]:|[F|f][W|w]:\",'',subject_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    tokens = nlp(sentence.strip())\n",
    "    \n",
    "    #Pre-processing of tokens\n",
    "    tokens = [token for token in tokens if token.pos_ not in pos_tokens_unused]\n",
    "    \n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    tokens = [token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_ for token in tokens]\n",
    "    \n",
    "    #Consider tokens that are not punctuations and if length of token is greater than 2 and if they are not stop words\n",
    "    tokens = [ token for token in tokens if token not in punctuations and len(token) > 2 and token not in spacy_stopwords]\n",
    "    \n",
    "    # return preprocessed list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing emails\n",
    "#Seperating individual emails to get the Current and any previous forwarded/orginal email messages and pre-processing\n",
    "emails_data['body_seperated']= emails_data.body.str.split('-----Original Message-----')\n",
    "emails_data['message_payloads'] = emails_data.body_seperated.apply(get_payloads)\n",
    "emails_data.subject = emails_data.subject.apply(handle_abbrevations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine name, subject and message for similarity and ranking\n",
    "emails_data['message'] = emails_data.message_payloads.apply(lambda sentence : ', '.join(sentence))\n",
    "emails_data['message_recieved'] = emails_data.message_payloads.apply(get_payloads,args=(True,))\n",
    "emails_data.message = emails_data.from_name + ' ' + emails_data.subject + ' ' + emails_data.message\n",
    "emails_data = emails_data.drop_duplicates(subset= 'message_recieved').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Out of Office Emails\n",
    "ooo_emails = emails_data[emails_data.subject.str.lower().str.contains(pat = 'out of the office|out of office',regex=True)].index\n",
    "emails_data = emails_data.drop(ooo_emails).reset_index()\n",
    "ooo_emails = emails_data[emails_data.message_recieved.str.lower().str.contains(pat = 'out of the office|out of office',regex=True)].index\n",
    "emails_data = emails_data.drop(ooo_emails).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF weighing for documents\n",
    "def generate_tfidf(messages,abbrevations,customer_tokenizer):\n",
    "    vectorizer = TfidfVectorizer(stop_words=None,smooth_idf=True,tokenizer=customer_tokenizer,sublinear_tf=True)\n",
    "    tfidf = vectorizer.fit_transform(messages)\n",
    "    query_abbrevations = vectorizer.transform(abbrevations)\n",
    "    query_from = vectorizer.transform(email_from)\n",
    "    return tfidf,query_abbrevations,query_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Cosine Similiary between the query and email\n",
    "def get_cosine_similarity(tfidf,query):\n",
    "    similarity = {}\n",
    "    for ind_query in range(query.get_shape()[0]):\n",
    "        for index in range(tfidf.get_shape()[0]):\n",
    "            if 'email_' + str(index) in similarity.keys():\n",
    "                similarity['email_' + str(index)]+= cosine_similarity(tfidf[index],query[ind_query])\n",
    "            else:\n",
    "                similarity['email_' + str(index)] = cosine_similarity(tfidf[index],query[ind_query])\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a list of top ranked emails based on the sender and content priority.\n",
    "def get_top_emails(similarity_dict_abbrevations,similarity_dict_from):\n",
    "    #Add up the weights to obtain the final ranking\n",
    "    final_similarity = Counter(similarity_dict_abbrevations) + Counter(similarity_dict_from)\n",
    "    top_emails_index = final_similarity.most_common(20)\n",
    "    top_emails = pd.DataFrame()\n",
    "    for email in top_emails_index:\n",
    "        email_no = int(email[0].split('_')[1])\n",
    "        top_emails = top_emails.append(emails_data.iloc[email_no])\n",
    "    top_emails = top_emails.reset_index(drop=True)\n",
    "    return top_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate TFIDF Sparse matrix\n",
    "tfidf,query_abbrevations,query_from = generate_tfidf(emails_data.message_recieved,email_abbrevations,customer_tokenizer)\n",
    "#Generate Cosine Similarity between the email abbrevations that are commonly used and the emails\n",
    "similarity_dict_abbrevations = get_cosine_similarity(tfidf,query_abbrevations)\n",
    "#Getting the mean and assigning the priority for the abbrevations\n",
    "for index in range(len(similarity_dict_abbrevations)):\n",
    "    similarity_dict_abbrevations['email_' + str(index)] = similarity_dict_abbrevations['email_' + str(index)] * abbrevation_priority\n",
    "#Generate Cosine Similarity between the email abbrevations that are commonly used and the emails\n",
    "similarity_dict_from = get_cosine_similarity(tfidf,query_from) \n",
    "#Getting the mean and assigning the priority for the abbrevations\n",
    "for index in range(len(similarity_dict_from)):\n",
    "    similarity_dict_from['email_' + str(index)] = similarity_dict_from['email_' + str(index)] * email_from_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_emails = get_top_emails(similarity_dict_abbrevations,similarity_dict_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the extracted sentences, get the vectors of the sentences\n",
    "def generate_sentence_vectors(sentences,word_embeddings):\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) != 0:\n",
    "            vector = sum([word_embeddings.get(word, np.zeros((100,))) for word in sentence.split()])/(len(sentence.split())+0.001)\n",
    "        else:\n",
    "            vector = np.zeros((100,)) \n",
    "        sentence_vectors.append(vector)\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the ranked sentences using Text Rank algorithm\n",
    "def generate_text_rank(sentence_vectors,sentences):\n",
    "    cosine_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                cosine_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    summary_graph = nx.from_numpy_array(cosine_mat)\n",
    "    text_rank = nx.pagerank(summary_graph)\n",
    "    ranked_sentences_with_scores = sorted(((text_rank[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    ranked_sentences = [sentence[1] for sentence in ranked_sentences_with_scores[:5]]\n",
    "    return ranked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the emails based on sentences and generate ranked sentences.\n",
    "def generate_summary(data,word_embeddings):\n",
    "    sentences = []\n",
    "    for sentence in data:\n",
    "        sentence =  sent_tokenize(sentence)\n",
    "        for line in sentence:\n",
    "            cleaned_sentence = ' '.join(customer_tokenizer(line))\n",
    "            sentences.append(cleaned_sentence)\n",
    "    sentence_vectors = generate_sentence_vectors(sentences,word_embeddings)\n",
    "    ranked_sentences = generate_text_rank(sentence_vectors,sentences)\n",
    "    return ' '.join(ranked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Ranked Emails\n",
      "\n",
      "Email 1\n",
      "\n",
      "request kim detiveaux attach propose form non disclosure agreement comment question mark greenberg senior counsel enclose meet approval execute agreement return fax look forward hear tana thank mark quick response\n",
      "**************************************************\n",
      "\n",
      "Email 2\n",
      "\n",
      "open trade correct tana second user apply company urgent thank camille\n",
      "**************************************************\n",
      "\n",
      "Email 3\n",
      "\n",
      "run mark taylor attorney legal dept attach clean blackline draft reflect change able lawyer need talk mark directly phone look forward hear think fine change tana jones send friday subject aether systems revise nda forward tana jones hou ect |--------+----------------------- tana jones |--------+----------------------- jgatto@aet.hersystems.com teresa smith corp enron@enron mark taylor hou ect@ect subject aether systems revise nda theresa smith forward comment nda\n",
      "**************************************************\n",
      "\n",
      "Email 4\n",
      "\n",
      "want document actually store online need contact moyez lallani access livelink database list find financial trading agreements database double click click open tell legal new york need access asap limor instruction help find access financial trading agreement database isda information locate select find double click legal folder\n",
      "**************************************************\n",
      "\n",
      "Email 5\n",
      "\n",
      "thank tana know project absolute priority list happy\n",
      "**************************************************\n",
      "\n",
      "Email 6\n",
      "\n",
      "request mark taylor enclose draft non disclosure agreement request assistance way hesitate counterparty execute agreement return file agreement accordance standard procedure thank quick turn file prediction company doc\n",
      "**************************************************\n",
      "\n",
      "Email 7\n",
      "\n",
      "forward tana jones hou ect leslie hansen tana jones hou ect@ect rhonda denton hou ect@ect subject eol credit response ltv steel company inc. authorize trade power restrict credit thank immediate attention oklahoma gas electric eol access oklahoma gas electric authorize trade power pursuant line gtc veronica tana jones veronica gonzalez hou ect@ect stephanie sever hou ect@ect subject eol credit response rush oklahoma great weekend\n",
      "**************************************************\n",
      "\n",
      "Email 8\n",
      "\n",
      "bryan change effective 5/24/01 word guy open canadian financial basis swaps check profile manager case thank bryan thank bryan\n",
      "**************************************************\n",
      "\n",
      "Email 9\n",
      "\n",
      "request attatche priority list eol emmisions auction\n",
      "**************************************************\n",
      "\n",
      "Email 10\n",
      "\n",
      "thank louis tana quick question receive pdf attachment mail \n",
      "**************************************************\n",
      "\n",
      "Email 11\n",
      "\n",
      "forward mark taylor hou ect raspe adele adele.raspe2@pseg.com subject electronic trading agmt document link mark taylor adele revise version mark ahead letter agreement execute revise send attention mark attach file jeffrey.foose@pseg.com hancock marcus marcus.hancock@pseg.com subject electronic trading agmt\n",
      "**************************************************\n",
      "\n",
      "Email 12\n",
      "\n",
      "thank attention matter tana thank bring attention apologize switch profile friday email group address migrate correctly regards keegan farrell keegan include member team distribution member swap group office revise group list reflect suggest change\n",
      "**************************************************\n",
      "\n",
      "Email 13\n",
      "\n",
      "forward mark taylor hou ect louise kitchen mark taylor hou ect@ect subject\n",
      "**************************************************\n",
      "\n",
      "Email 14\n",
      "\n",
      "let know date arrive notice possible thing ready robert quick mark ect legal taylor hou ect@ect subject secondment mark secondment come week time look forward come robert robert quick lon ect@ect subject secondment robert believe sylvia sauseda usually handle matter look forward\n",
      "**************************************************\n",
      "\n",
      "Email 15\n",
      "\n",
      "forward mark ect legal taylor hou ect mark ect legal taylor tana jones hou ect@ect subject gtc\n",
      "**************************************************\n",
      "\n",
      "Email 16\n",
      "\n",
      "training material click link need help time ipayit user receive message unresolved invoice ipayit box past remember play important role ensure pay vendor time launch ipayit click link note ipayit user password ehronline sap personnel password\n",
      "**************************************************\n",
      "\n",
      "Email 17\n",
      "\n",
      "forward mark taylor hou ect jarrod cyprow mark taylor hou ect@ect sara shackleton hou ect@ect edward ondarza hou ect@ect subject media general confirm attach final copy media general confirmation\n",
      "**************************************************\n",
      "\n",
      "Email 18\n",
      "\n",
      "alan enron north america corp. tana jones alan aronowitz hou ect@ect jeffrey hodge hou ect@ect stacy dickson hou ect@ect leslie hansen hou ect@ect harry collins hou ect@ect david portz hou ect@ect elizabeth sager hou ect@ect subject emmision contact list attach list counterpartie eol want approve emission auction trading forward tana jones hou ect eric tana jones hou ect@ect david forster hou ect@ect zal masani hou ect@ect subject emmision contact list request attatche priority list eol emmisions auction let know asap counterpartie list allow trade emission auction list look fine\n",
      "**************************************************\n",
      "\n",
      "Email 19\n",
      "\n",
      "follow james hardie enter lotus notes change consolidated papers inc. medium priority\n",
      "**************************************************\n",
      "\n",
      "Email 20\n",
      "\n",
      "information incorrect plan change mail sara davidson information rsvp law conference autoreply joseph.lippeatt@enron.com respond sara davidson tana jones subject rsvp law conference autoreply thank submit follow response rsvp online form enron law conference information submit follow attendance tana jones position legal specialist paralegal legal group ena phone fax email admin email arrival date departure date hotel accommodations double bed non smoking thursday activity select activity select golf handicap awards dinner sap company sap cost center message confirmation detail rsvp form got change\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "top_emails['email_summary'] = top_emails.message_payloads.apply(generate_summary,args=(word_embeddings,))\n",
    "print('Summary of Ranked Emails')\n",
    "for index,row in top_emails.iterrows():\n",
    "    print('\\nEmail {}\\n'.format(index+1))\n",
    "    print(row['email_summary'])\n",
    "    print('*'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
